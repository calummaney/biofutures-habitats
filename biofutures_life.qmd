---
title: "Implementing LIFE in Biofutures"
format: html
editor: visual
---

## Intro: LIFE under land-use scenarios from the NFF

The [LIFE metric](https://royalsocietypublishing.org/doi/10.1098/rstb.2023.0327) represents the impact of land cover change on species extinctions worldwide. It has been used so far to produce a "monitoring" style contemporary layer, and in broadly-defined scenarios of restoration and conservation.

We propose to apply these methods to "scenarios" – quantified visions of the future – based on global interpretations of the [Nature Futures Framework](https://www.pbl.nl/en/publications/report-on-the-workshop-next-steps-in-developing-nature-futures). This method, based in the LandSymm modelling framework, outlines three (plus one comparison scenario) scenarios of global land use where decisions relating to nature are made based on quintessential characterisations of how people value nature.

-   "Intrinsic", or "Nature for Nature" scenarios focus on a value perspective aiming to preserve the maximum diversity of life at the scale of decision-making, focusing on preserving eco-evolutionary novelty and reducing species extinctions.

-   "Instrumental" or "Nature for Society" scenarios focus on a value perspective related to the contributions nature makes to human society, through ecosystem services. Maintaining the functional health of ecosystems is of key importance here.

-   "Relational" or "Nature as Culture" scenarios focus not on the intrinsic value of nature, nor humanity's benefit from it, but peoples' *relational* experience of nature. This is harder to quantify, but includes culturally important species, ecosystems and landscapes, and the feeling of humanity as "part of " nature.

But how will implementing economic and land use decision-making based on these perspectives affect biodiversity worldwide? The LIFE metric can illustrate how changing patterns in land use could bring species closer to, or further away from, extinction related to land-use change.

We use the land use change data from PLUM simulations alongside a "habitat base layer" from Lumbierres et al. 2022 to create maps of changing habitat extents between scenarios.

## 0. Toolkit

We use the GBIF API to download 2010-2020 observations of many vertebrate species. We use the red list API to extract species-habitat associations. For land use, we use the *terra* package to hand the HILDA+ dataset.

```{r setup}
#Data handling
library(dplyr)
library(tidyr)
#API wrappers
library(rgbif)
library(rredlist)
#Handling spatial data
library(terra)
library(sf)
#Needed for manual GBIF API usage
library(httr)
library(jsonlite)

#Options for loading data
hilda_years <- paste(seq(2010,2019))

#TEST - NOT RUN
# aves <- occ_download_prep(
#   pred("taxonKey", 3119195),
#   pred_gt("elevation", 5000),
#   user = "cmaney",
#   pwd = "RC)Y}GHr.i8$YE8",
#   email = "calummaney@gmail.com")

myredlistkey <- 'fd3cbfc9044a98a1a44be6c867f633b05a984aab7f730ba304067120ca94f6f0'
```

## 1. Download datasets

### GBIF data on vertebrates

Follow the schema from the paper, we retrieve and filter GBIF observation data to train our models.

```{r gbifCall}
gbif_call <- occ_download(
  pred_or(
    pred("taxonKey", 212), #birds
    pred("taxonKey", 359), #mammals
    pred("taxonKey", 131), #amphibia
    pred("taxonKey", 358) #reptiles
  ),
  
  pred_gte("year", 2010), #year start
  pred_lte("year", 2019), #year end
  
  #Points need coordinates
  pred_not(pred_isnull("decimalLatitude")),
  pred_not(pred_isnull("decimalLongitude")),
  
  #Keep observations only
  pred("basisOfRecord", "HUMAN_OBSERVATION"),
  
  #Keep only low-uncertainty locations
  pred_lte("coordinateUncertaintyInMeters", 100),
  
  #Retrieve simplified data
  format = "SIMPLE_CSV",
  
  user = "cmaney",
  pwd = "RC)Y}GHr.i8$YE8",
  email = "calummaney@gmail.com")
```

Then we wait and pull the file once it has been generated by gbif.

```{r gbifDownloadWait}
occ_download_wait(gbif_call)
```

Download into a .zip file, save the path object so it can be called again later

```{r gbifDownloadExecute}
dir.create("raw_data/gbif/",showWarnings = F)

zip_file_path <- occ_download_get(key = "0025277-251025141854904", path = "raw_data/gbif/", overwrite = TRUE)

saveRDS(zip_file_path,"raw_data/gbif/zip_file_path.rds")
```

The last thing we do is to load the data from the .zip into an R object

### HILDA+ Land use

```{r downloadHILDA}

hilda_url <- "https://download.pangaea.de/dataset/921846/files/hildap_vGLOB-1.0_geotiff.zip"

hilda_zip <- download.file(hilda_url, destfile = "raw_data/hilda_plus.zip", method="curl")

unzip(zipfile = "raw_data/hilda_plus.zip",exdir = "raw_data/hilda/")

#Delete the .zip to tidy up
if (file.exists("raw_data/hilda_plus.zip")) {
  #Delete file if it exists
  file.remove("raw_data/hilda_plus.zip")
}
```

### IUCN Habitat code data

We load this locally so no download script here.

## 2. Pull GBIF species data from the IUCN Red List

Matching using names alone is likely to lead to errors. We can use the GBIF API again here to collect IUCN taxon IDs, before matching them to habitat associations.

```{r gbifUnzip}
zip_file_path <- readRDS("raw_data/gbif/zip_file_path.rds")

gbif_data <- occ_download_import(x = zip_file_path,path = ".") #est. 5 mins
```

Function to use the GBIF species API to link the GBIF backbone to the IUCN red list:

```{r}
iucn_hab <- read.csv("raw_data/Habitats/WCMC_Habitat_Info2_Aug2014_AA.csv")

getIUCNtaxonID <- function(gbif_taxonKey = gbif_data[1,]$taxonKey){
  res <- GET(paste0("http://api.gbif.org/v1/species/",gbif_taxonKey,"/iucnRedListCategory"))
  
  if(length(res$content)>0){
  
  Sys.sleep(runif(n = 1,min=0,max=100)/1000)
  
  res <- try(fromJSON(rawToChar(res$content))$iucnTaxonID)
  
  return(res)
  } else{
    return(NA)
  }
}
```

Apply the function to all the observed GBIF taxon codes:

```{r}
gbif_taxonKeys <- unique(gbif_data$taxonKey) 
```

```{r}
gbif_data_IUCN_codes <- lapply(gbif_taxonKeys,getIUCNtaxonID)

gbif_data_IUCN_codes[sapply(gbif_data_IUCN_codes, is.null)] <- NA

#Save the data
saveRDS(gbif_data_IUCN_codes,"wip/gbif_data_IUCN_codes.rds")
```

```{r}
gbif_data_IUCN_codes <- readRDS("wip/gbif_data_IUCN_codes.rds")

taxa.df <- data.frame(
  gbif_ID = gbif_taxonKeys,
  iucn_ID = gbif_data_IUCN_codes |> unlist()
)
```

## 3. Filter and clean, then join the datasets.

We can use a legacy dataset for testing.

```{r}
iucn_hab_suit <- iucn_hab[iucn_hab$suitability == "Suitable",]
iucn_hab_suit$taxonid <- paste(iucn_hab_suit$taxonid)

iucn_hab_suit <- iucn_hab_suit[,c("taxonid","friendly_name","hab_code","suitability")] |> unique.data.frame()
```

Join the gbif ID, iucn ID, and iucn habitat association datasets:

```{r}
taxa_iucn.df <- left_join(taxa.df,iucn_hab_suit,by=c("iucn_ID"="taxonid")) |>
  filter(suitability == "Suitable")
```

Pivot the habitat association data wider

```{r}
taxa_iucn_wide.df <- pivot_wider(taxa_iucn.df,names_from = hab_code,values_from = suitability)
```

Filter the observations data to only species we have habitat information for.

```{r}
gbif_withHabs <- gbif_data |> 
  filter(taxonKey %in% taxa_iucn.df$gbif_ID) |>
  transmute(
    gbifID,
    year,
    taxonKey,
    decimalLongitude,
    decimalLatitude
  )
```

Finally, join the observations data to the habitat associations data

```{r}
obs_habs.df <- gbif_withHabs |>
  left_join(taxa_iucn_wide.df, by = c("taxonKey"="gbif_ID"))
```

## 4. Extract land use data for the models

We have downloaded all the HILDA+ data, but to work with it we need to format it and filter it to the years we are using for training:

```{r prepareHILDA}
hilda_files <- lapply(hilda_years,function(x){list.files(path = "raw_data/hilda/hildap_vGLOB-1.0_geotiff_wgs84/hildap_GLOB-v1.0_lulc-states/",pattern = x,full.names = TRUE)}) |> unlist()

#Load and stack the path vector into one raster
hilda <- rast(hilda_files)

#Set the land use data to be categorical
hilda <- hilda |> as.factor() #this will take a while (est. 5-10 mins)

# HILDA+ levels:
# 00 ocean
# 11 urban
# 22 cropland
# 33 pasture/rangeland
# 44 forest
# 55 unmanaged grass/shrubland
# 66 sparse/no vegetation
# 77 water
# 99 no data

hilda_levels <- data.frame(
    ID = c(00,11,22,33,44,55,66,77,99),
    plum_category = c("Ocean","Urban","Cropland","Pasture/Rangeland","Forest", "Unmanaged grass/shrubland","Sparse/no vegetation","Water","No data")
  )

#Set names for the layers
names(hilda) <- paste0("X",hilda_years)

#Set the correct levels for each layer in the data
for(thisYr in names(hilda)){
 levels(hilda[[thisYr]]) <- hilda_levels
}

#Finished, prepared data:
hilda
```

We need a function that takes a given "HILDA" year, and extracts the value of the contemporary land use map for each of the observations in the GBIF dataset. We can make this highly parallel on the server.

```{r}
getLUfromObs <- function(obs, year = 2019){
  #Construct sf from the observation records for this year
  thisYear_obs <- obs[obs$year == year,]
  thisYear_obs.sf <- st_as_sf(thisYear_obs,coords=c("decimalLongitude","decimalLatitude"))
  
  thisLU <- extract(hilda[[paste0("X",year)]],thisYear_obs.sf)
  
  thisYear_obs.sf$HILDA_LU <- thisLU[,2]
  
  return(thisYear_obs.sf)
}

obs_habs_lu.sf <- lapply(hilda_years,function(yr){getLUfromObs(obs_habs.df,year=yr)}) |> bind_rows() #est. 3 min

plot(obs_habs_lu.sf[1:100000,]["HILDA_LU"])
```

## 5. Fit logistic regressions to make a crosswalk table IUCN:PLUM

For each habitat column in our data, we now have a list of observations where particular land use categories do, or do not, co-occur. For example, "forest" land use co-occurrence with "urban" habitats:

```{r}
table("LU"=obs_habs_lu.sf$HILDA_LU,"Habitat" = obs_habs_lu.sf$`14.5`)

table(obs_habs_lu.sf$HILDA_LU)
```

We can fit logistic regressions to identify where land use is predictive of habitat:

```{r}
testGlm <- glm(
  formula = !is.na(`14.5`) ~ HILDA_LU,
  data = obs_habs_lu.sf,
  family = "binomial"
)

testGlm
summary(testGlm)

testLU <- hilda$X2019
names(testLU) <- "HILDA_LU"

testPred <- predict(testLU,testGlm,type = "response") #est. 5-10 mins

plot(testPred)
```

## 6. Calculate habitat maps for each keystone PLUM year in scenarios

```{r}

```

## 4. Run LIFE methodology on the habitat maps

```{r}

```
